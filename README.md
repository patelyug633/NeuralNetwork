Neural Network from Scratch
A modular neural network implementation in Python built entirely from scratch without using deep learning frameworks. This project provides a flexible foundation for understanding and experimenting with core neural network concepts.

Features
Modular Layer Architecture: Easily create custom neural network layers

Flexible Network Composition: Stack layers to build various network architectures

Multiple Activation Functions: Sigmoid, ReLU, Tanh, and Softmax

Loss Functions: Mean Squared Error (MSE) and Cross-Entropy Loss

Optimizers: Stochastic Gradient Descent (SGD) with momentum

Forward & Backward Propagation: Full implementation of the backpropagation algorithm

Extensible Design: Easy to add new layer types, activations, and loss functions
